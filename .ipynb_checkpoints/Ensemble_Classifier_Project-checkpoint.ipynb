{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project_title",
   "metadata": {},
   "source": [
    "# Ensemble Classifier Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "import kagglehub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = kagglehub.dataset_download(\"vesuvius13/serial-killers-dataset\")\n",
    "dfs = []\n",
    "for file in ['Lessthan_5_victim_count.csv', '5_to_14_victim_count.csv', '15_to_30_victim_count.csv', 'Highest_victim_count.csv']:\n",
    "    df = pd.read_csv(f'{path}/{file}')\n",
    "    df['category'] = ['Low','Medium','High','Very High'][['Lessthan_5','5_to_14','15_to_30','Highest'].index([x for x in ['Lessthan_5','5_to_14','15_to_30','Highest'] if x in file][0])]\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "data['victims'] = data['Proven victims'].str.extract('(\\\\d+)').astype(float)\n",
    "data['years'] = data['Years active'].str.len()\n",
    "data = data.dropna(subset=['victims', 'years'])\n",
    "\n",
    "X = data[['victims', 'years']]\n",
    "y = data['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Data shape: {X.shape}, Classes: {y.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Models\n",
    "models = {\n",
    "    'RF': RandomForestClassifier(random_state=42),\n",
    "    'GB': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Baseline performance\n",
    "baseline = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    baseline[name] = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f'{name} Baseline: {baseline[name]:.3f}')\n",
    "\n",
    "# Voting Classifier\n",
    "voting = VotingClassifier([('rf', RandomForestClassifier()), ('gb', GradientBoostingClassifier())])\n",
    "voting.fit(X_train, y_train)\n",
    "baseline['Voting'] = accuracy_score(y_test, voting.predict(X_test))\n",
    "print(f'Voting: {baseline[\"Voting\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameter_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "params = {\n",
    "    'RF': {'n_estimators': [50, 100], 'max_depth': [5, 10]},\n",
    "    'GB': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]}\n",
    "}\n",
    "\n",
    "tuned = {}\n",
    "for name, model in models.items():\n",
    "    grid = GridSearchCV(model, params[name], cv=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    tuned[name] = {'model': grid.best_estimator_, 'score': accuracy_score(y_test, grid.predict(X_test))}\n",
    "    print(f'{name} Tuned: {tuned[name][\"score\"]:.3f}, Best params: {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_graphs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Graphs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "models_list = list(baseline.keys())\n",
    "baseline_scores = [baseline[m] for m in models_list]\n",
    "tuned_scores = [tuned.get(m, {}).get('score', baseline[m]) for m in models_list]\n",
    "\n",
    "x = range(len(models_list))\n",
    "plt.bar([i-0.2 for i in x], baseline_scores, 0.4, label='Baseline')\n",
    "plt.bar([i+0.2 for i in x], tuned_scores, 0.4, label='Tuned')\n",
    "plt.xticks(x, models_list)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "best_model = max(tuned.values(), key=lambda x: x['score'])['model']\n",
    "plt.bar(['victims', 'years'], best_model.feature_importances_)\n",
    "plt.title('Feature Importance')\n",
    "plt.ylabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=['victims', 'years'], show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "print('\\n=== RESULTS ===')\n",
    "print(f'Best model accuracy: {max(tuned.values(), key=lambda x: x[\"score\"])[\"score\"]:.3f}')\n",
    "print('Hyperparameter tuning output completed')\n",
    "print('SHAP explanations generated')\n",
    "print('Ensemble models compared successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}